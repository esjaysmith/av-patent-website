About the asking price: For us it’s not about the exact price and we look forward to your insights and your help in determining value. Our concern is the developing market of autonomous vehicles (AV) and its promising potential.
When my business partner and I started thinking about this in 2016 and incorporated our startup in 2017, AV were a dreamy idea for most people. Today, robo taxis and autonomous food delivery are already becoming a reality for some of us. And it seems a larger rollout of self driving vehicles is about to start, in the United States and elsewhere with Tesla FSD v12.

Autonomy is a software function and software yields higher margins than hardware. The obvious choice therefor is to license to software providers on a per vehicle instance and yearly basis. It’s not clear how many AV's we can expect in total in the future. Tesla’s FSD beta pool may currently be around 100k and perhaps 500k this time next year. Ten years from now it should have grown to more than 100 million, by their own reckoning. It will be interesting to see how things turn out, how many other players will go the camera/vision route and how many will  license self driving tech in order to better focus on vehicle production as opposed to developing their own AI software. And of course, light vehicle or drone delivery markets are growing, not to mention the very futuristic seeming investments in humanoid robots, can these even be characterised as AV's?

From the beginning, our conviction on how AV must be handled technically to be of use in the real world, has been vision based end to end deep learning. This was a deviation from the approach of (almost all of) the rest of the industry, having strong roots in robotics and its lidar and sensor driven map based navigation and object classification based obstacle detection. The need to better formulate our own ideas, and the wish to protect our investments, has led to our patents. The question we sought to answer is: How should an AV navigate, remain on the path it’s supposed to use and avoid obstacles, using only vision and without relying on overly detailed maps, which are also supposed to be always up to date?
The fact that Tesla FSD v12 exclusively uses camera fed neural networks trained end-end, now gives up hope we were betting on the right horse. We feel somewhat validated when comparing this approach to the efforts of Cruise, for example, and their difficulties of late.

Our startup has always been self funded and we have never sought out external funding. As a consequence we have not created much marketing material, such as investment decks, white papers, documents for the press, etc. This is why we do not have the kind of documentation that you naturally ask for.

As for why we are now offering the patents up for sale, there are two straightforward reasons.
Firstly, we were able to setup a business case in a rural area with customers and licenses from all local authorities, but we failed at obtaining the last regulatory approval necessary for applications in the Netherland from the central government. We feel this is due to our limited size as an organisation and the fact that available funds are capped.
Secondly, deep learning AI requires large quantities of data and therefor compute in order to function well, but to a much larger agree than we initially thought. The flip side being that when enough data is collected and processed, the functionality of neural nets and their generalisation ability seems to likewise grow, surprisingly so as exemplified by the recent advancements of large language models. But the scale required is well outside the scope of what we can hope to accomplish.
In short: The large investment required for a business case with regulatory approval in the Netherlands makes it unattainable for us.

This is a short clip with our technology in action:
- https://www.youtube.com/watch?v=SOWuOP-0nDE

The clip is a screen recording of the remote teleoperator view of the front (and also rear) camera, mounted on one of our small light weight electric research vehicles. For the duration of this clip the vehicle is fully autonomous and driving around in circles in the garden behind an office building.
The same teleoperator software can also be used to record new data during driving sessions or incremental updates of complex situations.
The somewhat grainy quality of the image is a result of compression, adaptive to the quality of the remote internet connection between the vehicle and the laptop of the operator.
The operator screen is divided into two main sections, a header on top and the main view that renders the active camera stream, usually the front camera.
The left of the header shows a thumbnail view of the other camera stream, usually the rear camera. The middle of the header is used to display current and desired speed and shows a steering wheel in different colours that represents the vehicle’s current controller, the operator or autopilot. The right of the header is used to show the most recently recognised navigation point (waypoint) and the next expected one, of the currently selected navigation route. A navigation point is an image or set of images that define a location at a specific orientation.
The software for teleoperation and autopilot has been developed by us, as well as the deep learning neural network model architecture, data labeling and training processes.
The network is trained end to end. It’s architectural beginning is imitation learning, e.g. Nvidia Dave2 2016. To this we added path planning by visual prompting (our method), free passage and model uncertainty (our method), and ‘upside-down’ reinforcement learning (udrl Schmidhuber 2022).
The resulting model is images in and controls out. Maps or other sensors are not used. The model navigates by determining the direction necessary to reach the next expected navigation point (image or group of images), steers around obstacles or stops when blocked or when runtime circumstances deviate too much from the training distribution (uncertainty).
The path visualisation as seen in the clip is a representation of the current direction, a function of the vehicle heading and next desired visual waypoint.
Free passage a function of known obstacles, known absence of any obstacles and absence of uncertainty. Object classification, localisation etc is not used or required.

We hope this helps give you a better insight into our approach.